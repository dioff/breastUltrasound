回归可以用于预测多少的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。

事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”：

- 某个电子邮件是否属于垃圾邮件文件夹？

- 某个用户可能注册或不注册订阅服务？

- 某个图像描绘的是驴、狗、猫、还是鸡？

- 某人接下来最有可能看哪部电影？

通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

## 分类问题
我们从一个图像分类问题开始。 假设每次输入是一个2x2的灰度图像。 我们可以用一个标量表示每个像素值，每个图像对应四个特征$x_1,x_2,x_3,x_4$。此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。

接下来，我们要选择如何表示标签。 我们有两个明显的选择：最直接的想法是选择 y ∈ {1，2，3}， 其中整数分别代表y ∈ {狗，猫，鸡}
这是在计算机上存储此类信息的有效方法。

如果类别间有一些自然顺序， 比如说我们试图预测{婴儿, 儿童,青少年, 青年人, 中年人, 老年人}那么将这个问题转变为回归问题，并且保留这种格式是有意义的。

但是一般的分类问题并不与类别之间的自然顺序有关。 幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：独热编码（one-hot encoding）。 独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。 在我们的例子中，标签y将是一个三维向量， 其中(1, 0, 0)对应于“狗”、(0, 1, 0)对应于“鸡”、(0, 0, 1)对应于“狗”：
    y ∈ {(1, 0, 0), (0, 1, 0), (0, 0, 1)}

## 网络架构
为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function）。 每个输出对应于它自己的仿射函数。 在我们的例子中，由于我们有4个特征和3个可能的输出类别， 我们将需要12个标量来表示权重（带下标的 $\omega$）， 3个标量来表示偏置（带下标的b）下面我们为每个输入计算三个未规范化的预测（logit）：$\omicron_1 \omicron_2  \omicron_3$

$$
\omicron_1 = \omega_{11}x_1 + \omega_{12}x_2 + \omega_{13}x_3 + \omega_{14}x_4 + b_1
$$
$$
\omicron_2 = \omega_{21}x_1 + \omega_{22}x_2 + \omega_{23}x_3 + \omega_{24}x_4 + b_2
$$
$$
\omicron_3 = \omega_{31}x_1 + \omega_{32}x_2 + \omega_{33}x_3 + \omega_{34}x_4 + b_3
$$

## Softmax运算
现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。我们希望模型的输出 $\hat{y}_j$ 可以视为属于类j的概率， 然后选择具有最大输出值的类别 $argmax_jy_j$作为我们的预测。例如，如果$\hat{y}_1 = 0.1$，$\hat{y}_2 = 0.6$，$\hat{y}_3 = 0.3$，那么我们预测的类别是2,在我们的例子中代表“鸡”。

然而我们能否将未规范化的预测 $\omicron$ 直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。 这些违反了概率基本公理。

要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做校准（calibration）。

社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上 发明的softmax函数正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：

$$
\hat{y} = softmax(\omicron) = \frac{exp(\omicron_j)}{\sum_{k} exp(\omicron_k)}
$$

这里对所有j总有 $0 \le \hat{y} \le 1$ ，因此 $\hat{y}$ 可以视为一个正确的概率分布。 softmax运算不会改变未规范化的预测 $\omicron$ 之间的大小次序，只会确定分配给每个类别的概率。 因此，在预测过程中，我们可以用下式来选择类别：

$$  
\hat{y}_j = \argmax_{j} \omicron_j
$$
尽管softmax是一个非线性函数，但softmax回归输出的仍然是由输入特征的仿射变化决定的。 因此，softmax回归仍然是线性模型。